{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "#mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_commonvoice_item(\n",
    "    line: List[str], header: List[str], path: str, folder_audio: str, ext_audio: str\n",
    ") -> Tuple[Tensor, int, Dict[str, str]]:\n",
    "    # Each line as the following data:\n",
    "    # client_id, path, sentence, up_votes, down_votes, age, gender, accent\n",
    "\n",
    "    if header[1] != \"path\":\n",
    "        raise ValueError(f\"expect `header[1]` to be 'path', but got {header[1]}\")\n",
    "    fileid = line[1]\n",
    "    filename = os.path.join(path, folder_audio, fileid)\n",
    "    if not filename.endswith(ext_audio):\n",
    "        filename += ext_audio\n",
    "    waveform, sample_rate = torchaudio.load(filename)\n",
    "\n",
    "    dic = dict(zip(header, line))\n",
    "\n",
    "    return waveform, sample_rate, dic\n",
    "\n",
    "\n",
    "class COMMONVOICE(Dataset):\n",
    "    \"\"\"*CommonVoice* :cite:`ardila2020common` dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str or Path): Path to the directory where the dataset is located.\n",
    "             (Where the ``tsv`` file is present.)\n",
    "        tsv (str, optional):\n",
    "            The name of the tsv file used to construct the metadata, such as\n",
    "            ``\"train.tsv\"``, ``\"test.tsv\"``, ``\"dev.tsv\"``, ``\"invalidated.tsv\"``,\n",
    "            ``\"validated.tsv\"`` and ``\"other.tsv\"``. (default: ``\"train.tsv\"``)\n",
    "    \"\"\"\n",
    "\n",
    "    _ext_txt = \".txt\"\n",
    "    _ext_audio = \".mp3\"\n",
    "    _folder_audio = \"clips\"\n",
    "\n",
    "    def __init__(self, root: Union[str, Path], tsv: str = \"train.tsv\") -> None:\n",
    "\n",
    "        # Get string representation of 'root' in case Path object is passed\n",
    "        self._path = os.fspath(root)\n",
    "        self._tsv = os.path.join(self._path, tsv)\n",
    "\n",
    "        with open(self._tsv, \"r\") as tsv_:\n",
    "            walker = csv.reader(tsv_, delimiter=\"\\t\")\n",
    "            self._header = next(walker)\n",
    "            self._walker = list(walker)\n",
    "\n",
    "    def __getitem__(self, n: int) -> Tuple[Tensor, int, Dict[str, str]]:\n",
    "        \"\"\"Load the n-th sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            n (int): The index of the sample to be loaded\n",
    "\n",
    "        Returns:\n",
    "            Tuple of the following items;\n",
    "\n",
    "            Tensor:\n",
    "                Waveform\n",
    "            int:\n",
    "                Sample rate\n",
    "            Dict[str, str]:\n",
    "                Dictionary containing the following items from the corresponding TSV file;\n",
    "\n",
    "                * ``\"client_id\"``\n",
    "                * ``\"path\"``\n",
    "                * ``\"sentence\"``\n",
    "                * ``\"up_votes\"``\n",
    "                * ``\"down_votes\"``\n",
    "                * ``\"age\"``\n",
    "                * ``\"gender\"``\n",
    "                * ``\"accent\"``\n",
    "        \"\"\"\n",
    "        line = self._walker[n]\n",
    "        return load_commonvoice_item(line, self._header, self._path, self._folder_audio, self._ext_audio)\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._walker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonvoice = COMMONVOICE(root=os.getcwd()+\"/data/cv-corpus-7.0-singleword/fr\", tsv=\"train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def extract_features(audio_path):\n",
    "    audio, sample_rate = librosa.load(audio_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.8731464e+02 -6.8731464e+02 -6.8731464e+02 ... -5.5644818e+02\n",
      "  -5.9824695e+02 -6.8721765e+02]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  1.1173247e+02\n",
      "   8.6702301e+01  1.3684773e-01]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  3.7776123e+01\n",
      "   3.3123108e+01  1.3596201e-01]\n",
      " ...\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  1.8497684e+01\n",
      "   1.8942513e+01  1.0864532e-01]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  9.8468342e+00\n",
      "   1.4657414e+01  1.0294381e-01]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  6.0690675e+00\n",
      "   8.5757990e+00  9.6813396e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(extract_features(\"./data/cv-corpus-7.0-singleword/fr/clips/common_voice_fr_21894151.mp3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri /Users/robinbochu/Documents/School/TelecomSaintEtienne/TSE3/Apprentissage automatique/projet_big_data/data/cv-corpus-7.0-singleword/fr/clips/common_voice_fr_22018669.mp3 and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CommonVoiceDataset(tsv_file\u001b[38;5;241m=\u001b[39mtsv_file_train, clips_folder\u001b[38;5;241m=\u001b[39mclips_folder)\n\u001b[1;32m     43\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[138], line 23\u001b[0m, in \u001b[0;36mCommonVoiceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[1;32m     22\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclips_folder, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Apply transformations if any\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torchaudio/_backend/utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torchaudio/_backend/utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri /Users/robinbochu/Documents/School/TelecomSaintEtienne/TSE3/Apprentissage automatique/projet_big_data/data/cv-corpus-7.0-singleword/fr/clips/common_voice_fr_22018669.mp3 and format None."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import os\n",
    "\n",
    "class CommonVoiceDataset(Dataset):\n",
    "    def __init__(self, tsv_file, clips_folder, transform=None):\n",
    "        self.clips_folder = clips_folder\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        with open(tsv_file, newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                self.data.append(row)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        audio_path = os.path.join(self.clips_folder, row['path'])\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        \n",
    "        # Here, you could also include feature extraction steps if necessary\n",
    "        label = row['sentence']  # Or however you plan to use the label\n",
    "\n",
    "        return waveform, sample_rate, label\n",
    "\n",
    "\n",
    "# Example usage\n",
    "tsv_file_train = f\"{os.getcwd()}/data/cv-corpus-7.0-singleword/fr/train.tsv\"\n",
    "tsv_file_test = f\"{os.getcwd()}/data/cv-corpus-7.0-singleword/fr/test.tsv\"\n",
    "\n",
    "clips_folder = f\"{os.getcwd()}/data/cv-corpus-7.0-singleword/fr/clips/\"\n",
    "\n",
    "dataset = CommonVoiceDataset(tsv_file=tsv_file_train, clips_folder=clips_folder)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for waveform, sample_rate, label in data_loader:\n",
    "    print(waveform.shape, sample_rate, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mettre nos dataset dans des dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([145152]) 48000 Firefox\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -5.0598e-06,\n",
      "        -1.0700e-05, -2.6028e-05])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "\n",
    "class CommonVoiceDataset(Dataset):\n",
    "    def __init__(self, tsv_path, audio_dir):\n",
    "        self.audio_dir = audio_dir\n",
    "        \n",
    "        # Read the TSV file and store the data\n",
    "        self.data = []\n",
    "        with open(tsv_path, 'r', encoding='utf-8') as f:\n",
    "            next(f)  # Skip the header\n",
    "            for line in f:\n",
    "                line_content = line.strip().split('\\t')\n",
    "                self.data.append({\n",
    "                    \"client_id\": line_content[0],\n",
    "                    \"path\": line_content[1],\n",
    "                    \"sentence\": line_content[2],\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        audio_path = os.path.join(self.audio_dir, item['path'])\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=40)\n",
    "        mfcc_tensor = torch.from_numpy(mfcc).float()\n",
    "        # Ensure 3D tensor for CNN (channel, MFCC features, time)\n",
    "        mfcc_tensor = mfcc_tensor.unsqueeze(0)\n",
    "        \n",
    "        label = item['sentence']  # Here, convert to appropriate label format as needed\n",
    "        \n",
    "        return mfcc_tensor, label\n",
    "\n",
    "# Specify the TSV file path and audio directory path\n",
    "train_tsv_file_path = './data/cv-corpus-7.0-singleword/fr/train.tsv'\n",
    "test_tsv_file_path = './data/cv-corpus-7.0-singleword/fr/test.tsv'\n",
    "\n",
    "audio_dir_path = './data/cv-corpus-7.0-singleword/fr/clips'\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = CommonVoiceDataset(train_tsv_file_path, audio_dir_path)\n",
    "test_dataset = CommonVoiceDataset(test_tsv_file_path, audio_dir_path)\n",
    "\n",
    "\n",
    "# Example: Access the first item in the dataset\n",
    "waveform_tensor, sample_rate, label = train_dataset[0]\n",
    "print(waveform_tensor.shape, sample_rate, label)\n",
    "\n",
    "print(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "shuffle = True\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        # Define your CNN architecture here\n",
    "        # Example:\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.fc1 = nn.Linear(32 * (number_of_mfcc_features // 4) * (time_steps // 4), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (8) to match target batch_size (4).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 128\u001b[0m\n\u001b[1;32m    126\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()   \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[1;32m    127\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(mfccs)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m    129\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m    130\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (8) to match target batch_size (4)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "class CommonVoiceDataset(Dataset):\n",
    "    def __init__(self, tsv_path, audio_dir, label_to_index):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.label_to_index = label_to_index  # Add this line\n",
    "        self.data = []\n",
    "        with open(tsv_path, 'r', encoding='utf-8') as f:\n",
    "            next(f)  # Skip header\n",
    "            for line in f:\n",
    "                line_content = line.strip().split('\\t')\n",
    "                self.data.append((line_content[1], self.label_to_index[line_content[2]]))  # Convert label to index here\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, label = self.data[idx]\n",
    "        audio_path_full = os.path.join(self.audio_dir, audio_path)\n",
    "        waveform, sample_rate = librosa.load(audio_path_full, sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=40)\n",
    "        mfcc_tensor = torch.from_numpy(mfcc).float()\n",
    "        mfcc_tensor = mfcc_tensor.unsqueeze(0)  # Add channel dimension\n",
    "        return mfcc_tensor, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Unpack the batch\n",
    "    mfccs, labels = zip(*batch)\n",
    "    \n",
    "    # Label encoding: Convert string labels to integers\n",
    "    # For demonstration, let's assume your labels are already integer-encoded\n",
    "    # If you have string labels, you should convert them to integers using a mapping dictionary\n",
    "    # label_to_index = {'label1': 0, 'label2': 1, ...}\n",
    "    # labels = [label_to_index[label] for label in labels]\n",
    "    \n",
    "    # Find the longest sequence (time dimension)\n",
    "    max_length = max(mfcc.size(2) for mfcc in mfccs)\n",
    "    # Find the max number of features (MFCC dimension)\n",
    "    max_features = max(mfcc.size(1) for mfcc in mfccs)\n",
    "    \n",
    "    # Pad each MFCC tensor to the max length and features\n",
    "    mfccs_padded = torch.zeros((len(mfccs), 1, max_features, max_length))\n",
    "    for i, mfcc in enumerate(mfccs):\n",
    "        # Determine padding sizes\n",
    "        pad_feature_dim = max_features - mfcc.size(1)\n",
    "        pad_time_dim = max_length - mfcc.size(2)\n",
    "        # Pad the MFCC tensor and assign to the padded batch\n",
    "        mfccs_padded[i] = nn.functional.pad(mfcc, (0, pad_time_dim, 0, pad_feature_dim))\n",
    "    \n",
    "    # Convert labels to tensor\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return mfccs_padded, labels\n",
    "\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, num_classes=14):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1)\n",
    "        \n",
    "        # Dynamically determine the flattened size after conv/pool layers\n",
    "        self._to_linear = None\n",
    "        self.conv_output_size((1, 40, 173))  # Adjust the shape based on your input data\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def conv_output_size(self, shape):\n",
    "        with torch.no_grad():\n",
    "            # Initialize a mock input based on the expected input shape\n",
    "            mock_input = torch.rand(1, *shape)  # Shape is (channels, MFCC features, time)\n",
    "            output = self.conv1(mock_input)\n",
    "            output = self.pool(output)\n",
    "            output = self.conv2(output)\n",
    "            output = self.pool(output)\n",
    "            # Compute the total number of features for the first linear layer\n",
    "            self._to_linear = int(np.prod(output.size()[1:]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self._to_linear)  # Flatten for FC\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have the following paths set up\n",
    "tsv_path = './data/cv-corpus-7.0-singleword/fr/train.tsv'\n",
    "audio_dir = './data/cv-corpus-7.0-singleword/fr/clips'\n",
    "\n",
    "train_tsv_file_path = './data/cv-corpus-7.0-singleword/fr/train.tsv'\n",
    "test_tsv_file_path = './data/cv-corpus-7.0-singleword/fr/test.tsv'\n",
    "\n",
    "audio_dir_path = './data/cv-corpus-7.0-singleword/fr/clips'\n",
    "\n",
    "# Example label to index mapping\n",
    "label_to_index = {'Firefox': 0, 'oui': 1, 'non': 2, 'un': 3, 'deux': 4, 'trois': 5,'quatre': 6,'cinq': 7,'six': 8,'sept': 9,'huit': 10,'neuf':11,'Hey':12,'z√©ro':13}\n",
    "\n",
    "# Update dataset initialization\n",
    "dataset = CommonVoiceDataset(tsv_path, audio_dir, label_to_index)\n",
    "\n",
    "# Instantiate dataset and data loader\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "num_classes = 14 # Replace with your actual number of classes\n",
    "model = AudioCNN(num_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1):  # Change this to the actual number of epochs\n",
    "    for mfccs, labels in data_loader:\n",
    "        optimizer.zero_grad()   # Zero the gradients\n",
    "        outputs = model(mfccs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        # Print loss\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
