{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de notre donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>locale</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89fa8146b2c07e3e2fe2bbf3852ed8b22a625e383df7eb...</td>\n",
       "      <td>common_voice_fr_21982258.mp3</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>teens</td>\n",
       "      <td>male</td>\n",
       "      <td>france</td>\n",
       "      <td>fr</td>\n",
       "      <td>Benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89fa8146b2c07e3e2fe2bbf3852ed8b22a625e383df7eb...</td>\n",
       "      <td>common_voice_fr_21982260.mp3</td>\n",
       "      <td>cinq</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>teens</td>\n",
       "      <td>male</td>\n",
       "      <td>france</td>\n",
       "      <td>fr</td>\n",
       "      <td>Benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89fa8146b2c07e3e2fe2bbf3852ed8b22a625e383df7eb...</td>\n",
       "      <td>common_voice_fr_21982262.mp3</td>\n",
       "      <td>trois</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>teens</td>\n",
       "      <td>male</td>\n",
       "      <td>france</td>\n",
       "      <td>fr</td>\n",
       "      <td>Benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89fa8146b2c07e3e2fe2bbf3852ed8b22a625e383df7eb...</td>\n",
       "      <td>common_voice_fr_21982265.mp3</td>\n",
       "      <td>deux</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>teens</td>\n",
       "      <td>male</td>\n",
       "      <td>france</td>\n",
       "      <td>fr</td>\n",
       "      <td>Benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89fa8146b2c07e3e2fe2bbf3852ed8b22a625e383df7eb...</td>\n",
       "      <td>common_voice_fr_21982266.mp3</td>\n",
       "      <td>sept</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>teens</td>\n",
       "      <td>male</td>\n",
       "      <td>france</td>\n",
       "      <td>fr</td>\n",
       "      <td>Benchmark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           client_id  \\\n",
       "0  89fa8146b2c07e3e2fe2bbf3852ed8b22a625e383df7eb...   \n",
       "1  89fa8146b2c07e3e2fe2bbf3852ed8b22a625e383df7eb...   \n",
       "2  89fa8146b2c07e3e2fe2bbf3852ed8b22a625e383df7eb...   \n",
       "3  89fa8146b2c07e3e2fe2bbf3852ed8b22a625e383df7eb...   \n",
       "4  89fa8146b2c07e3e2fe2bbf3852ed8b22a625e383df7eb...   \n",
       "\n",
       "                           path sentence  up_votes  down_votes    age gender  \\\n",
       "0  common_voice_fr_21982258.mp3  Firefox         2           0  teens   male   \n",
       "1  common_voice_fr_21982260.mp3     cinq         3           0  teens   male   \n",
       "2  common_voice_fr_21982262.mp3    trois         3           0  teens   male   \n",
       "3  common_voice_fr_21982265.mp3     deux         3           2  teens   male   \n",
       "4  common_voice_fr_21982266.mp3     sept         2           0  teens   male   \n",
       "\n",
       "   accent locale    segment  \n",
       "0  france     fr  Benchmark  \n",
       "1  france     fr  Benchmark  \n",
       "2  france     fr  Benchmark  \n",
       "3  france     fr  Benchmark  \n",
       "4  france     fr  Benchmark  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/cv-corpus-7.0-singleword/fr/train.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donnée balanced ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence\n",
       "cinq       622\n",
       "deux       621\n",
       "neuf       621\n",
       "non        621\n",
       "zéro       621\n",
       "oui        620\n",
       "quatre     620\n",
       "sept       620\n",
       "trois      618\n",
       "un         617\n",
       "six        614\n",
       "Firefox    611\n",
       "Hey        611\n",
       "huit       611\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cinq', 'deux', 'neuf', 'non', 'zéro', 'oui', 'quatre', 'sept', 'trois', 'un', 'six', 'Firefox', 'Hey', 'huit']\n"
     ]
    }
   ],
   "source": [
    "labels = list(df.value_counts('sentence').index)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trop de donnée\n",
    "Nous ne voulons que les lignes qui contiennent les valeurs: oui, non, un, deux, trois, quatre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deux', 'non', 'oui', 'quatre', 'trois', 'un']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "df = df[df['sentence'].isin(['un', 'deux', 'trois', 'quatre','oui','non'])]\n",
    "labels = list(df.value_counts('sentence').index)\n",
    "print(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui  va chercher les fichiers audio dans clips depuis les infos du tsv et les convertir en tensor\n",
    "def load_commonvoice_item(\n",
    "    line: List[str], header: List[str], path: str, folder_audio: str, ext_audio: str\n",
    ") -> Tuple[Tensor, int, Dict[str, str]]:\n",
    "    # Each line as the following data:\n",
    "    # client_id, path, sentence, up_votes, down_votes, age, gender, accent\n",
    "\n",
    "    if header[1] != \"path\":\n",
    "        raise ValueError(f\"expect `header[1]` to be 'path', but got {header[1]}\")\n",
    "    fileid = line[1]\n",
    "    filename = os.path.join(path, folder_audio, fileid)\n",
    "    if not filename.endswith(ext_audio):\n",
    "        filename += ext_audio\n",
    "    waveform, sample_rate = torchaudio.load(filename)\n",
    "\n",
    "    dic = dict(zip(header, line))\n",
    "\n",
    "    return waveform, sample_rate, dic\n",
    "\n",
    "\n",
    "# Classe qui va permettre de charger les données de CommonVoice\n",
    "class COMMONVOICE(Dataset):\n",
    "    \"\"\"*CommonVoice* :cite:`ardila2020common` dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str or Path): Path to the directory where the dataset is located.\n",
    "             (Where the ``tsv`` file is present.)\n",
    "        tsv (str, optional):\n",
    "            The name of the tsv file used to construct the metadata, such as\n",
    "            ``\"train.tsv\"``, ``\"test.tsv\"``, ``\"dev.tsv\"``, ``\"invalidated.tsv\"``,\n",
    "            ``\"validated.tsv\"`` and ``\"other.tsv\"``. (default: ``\"train.tsv\"``)\n",
    "    \"\"\"\n",
    "\n",
    "    _ext_txt = \".txt\"\n",
    "    _ext_audio = \".mp3\"\n",
    "    _folder_audio = \"clips\"\n",
    "\n",
    "    def __init__(self, root: Union[str, Path], tsv: str = \"train.tsv\", transform=None) -> None:\n",
    "\n",
    "        # Get string representation of 'root' in case Path object is passed\n",
    "        self._path = os.fspath(root)\n",
    "        self._tsv = os.path.join(self._path, tsv)\n",
    "\n",
    "        with open(self._tsv, \"r\") as tsv_:\n",
    "            walker = csv.reader(tsv_, delimiter=\"\\t\")\n",
    "            self._header = next(walker)\n",
    "            self._walker = list(walker)\n",
    "\n",
    "    def __getitem__(self, n: int) -> Tuple[Tensor, int, str]:\n",
    "        \"\"\"Load the n-th sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            n (int): The index of the sample to be loaded\n",
    "\n",
    "        Returns:\n",
    "            Tuple of the following items;\n",
    "\n",
    "            Tensor:\n",
    "                Waveform\n",
    "            int:\n",
    "                Sample rate\n",
    "            Dict[str, str]:\n",
    "                Dictionary containing the following items from the corresponding TSV file;\n",
    "\n",
    "                * ``\"client_id\"``\n",
    "                * ``\"path\"``\n",
    "                * ``\"sentence\"``\n",
    "                * ``\"up_votes\"``\n",
    "                * ``\"down_votes\"``\n",
    "                * ``\"age\"``\n",
    "                * ``\"gender\"``\n",
    "                * ``\"accent\"``\n",
    "        \"\"\"\n",
    "        line = self._walker[n]\n",
    "        waveform, sample_rate, dic = load_commonvoice_item(line, self._header, self._path, self._folder_audio, self._ext_audio)\n",
    "        spectrogram = transform(waveform)\n",
    "        length = spectrogram.shape[-1]  # Time dimension length\n",
    "        label = dic[\"sentence\"]\n",
    "        print(label)\n",
    "        return spectrogram, length, label\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._walker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinbochu/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, FrequencyMasking, TimeMasking\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_sequence\n",
    "\n",
    "\n",
    "audio_transforms = nn.Sequential(\n",
    "    T.MelSpectrogram(sample_rate=48000)\n",
    ")\n",
    "\n",
    "commonvoice = COMMONVOICE(root=\"./data/cv-corpus-7.0-singleword/fr\", tsv=\"train.tsv\",transform=audio_transforms)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Sort the batch in the descending order of sequence length for packing\n",
    "    sorted_batch = sorted(batch, key=lambda x: x[1], reverse=True)\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    lengths = torch.tensor([x[1] for x in sorted_batch])\n",
    "    labels = [x[2] for x in sorted_batch]  # Collect labels\n",
    "\n",
    "    # Depending on your model, you may pack sequences or pad them\n",
    "    packed_sequences = pack_sequence(sequences, enforce_sorted=True)\n",
    "    # If your labels are tensor-like and you're using something like CrossEntropyLoss, \n",
    "    # you might not need to pad or pack labels. Otherwise, adapt as necessary.\n",
    "    \n",
    "    # If labels need to be a tensor (e.g., for classification with CrossEntropyLoss),\n",
    "    # ensure labels are converted to a tensor. This assumes labels are numeric.\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return packed_sequences, lengths, labels\n",
    "\n",
    "\n",
    "# # Assuming `commonvoice` is your dataset instanc\n",
    "commonvoice_loader = DataLoader(commonvoice, batch_size=64, shuffle=True,drop_last=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sept\n",
      "non\n",
      "quatre\n",
      "six\n",
      "neuf\n",
      "Hey\n",
      "neuf\n",
      "oui\n",
      "Firefox\n",
      "sept\n",
      "Hey\n",
      "trois\n",
      "oui\n",
      "oui\n",
      "un\n",
      "Firefox\n",
      "un\n",
      "cinq\n",
      "zéro\n",
      "quatre\n",
      "neuf\n",
      "trois\n",
      "oui\n",
      "oui\n",
      "deux\n",
      "non\n",
      "huit\n",
      "huit\n",
      "un\n",
      "Hey\n",
      "cinq\n",
      "Hey\n",
      "six\n",
      "quatre\n",
      "oui\n",
      "trois\n",
      "non\n",
      "sept\n",
      "deux\n",
      "non\n",
      "oui\n",
      "six\n",
      "Firefox\n",
      "cinq\n",
      "cinq\n",
      "zéro\n",
      "huit\n",
      "cinq\n",
      "neuf\n",
      "non\n",
      "deux\n",
      "sept\n",
      "un\n",
      "quatre\n",
      "Firefox\n",
      "trois\n",
      "oui\n",
      "neuf\n",
      "trois\n",
      "trois\n",
      "trois\n",
      "huit\n",
      "deux\n",
      "trois\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1106) must match the size of tensor b (1014) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[241], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Adjust sizes as needed\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpacked_sequences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcommonvoice_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Shape should be (num_layers, batch_size, hidden_size)\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_sequences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[239], line 23\u001b[0m, in \u001b[0;36mcustom_collate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     20\u001b[0m labels \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sorted_batch]  \u001b[38;5;66;03m# Collect labels\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Depending on your model, you may pack sequences or pad them\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m packed_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mpack_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# If your labels are tensor-like and you're using something like CrossEntropyLoss, \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# you might not need to pad or pack labels. Otherwise, adapt as necessary.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# If labels need to be a tensor (e.g., for classification with CrossEntropyLoss),\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# ensure labels are converted to a tensor. This assumes labels are numeric.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/nn/utils/rnn.py:484\u001b[0m, in \u001b[0;36mpack_sequence\u001b[0;34m(sequences, enforce_sorted)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Packs a list of variable length Tensors.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03mConsecutive call of the next functions: ``pad_sequence``, ``pack_padded_sequence``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    a :class:`PackedSequence` object\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor([v\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m sequences])\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_padded_sequence(\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m, lengths, enforce_sorted\u001b[38;5;241m=\u001b[39menforce_sorted)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/nn/utils/rnn.py:399\u001b[0m, in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    395\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1106) must match the size of tensor b (1014) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, packed_sequences):\n",
    "        # LSTM can directly consume packed sequences\n",
    "        packed_output, (ht, ct) = self.lstm(packed_sequences)\n",
    "        # Do something with the output...\n",
    "        return ht\n",
    "\n",
    "# Example usage\n",
    "model = MyModel(input_size=128, hidden_size=256, num_layers=2)  # Adjust sizes as needed\n",
    "for packed_sequences in commonvoice_loader:\n",
    "    output = model(packed_sequences)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
